{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TFiPS_C3W1Lesson2_Tokenizer_TextToSequences_Padding.ipynb","provenance":[{"file_id":"13ecLdfnNN7gYrbEfG9F8LSiKXbTXoGem","timestamp":1584915464363}],"collapsed_sections":[],"authorship_tag":"ABX9TyPrj92xF22p6XJV25V9LAPl"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"OQsr48n3y4k1","colab_type":"text"},"source":["Coursera link: https://www.coursera.org/learn/natural-language-processing-tensorflow/lecture/Sydkf/notebook-for-lesson-2\n","\n","Keras Text Preprocessing: https://keras.io/preprocessing/text/\n","\n","The same in TF library: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer"]},{"cell_type":"markdown","metadata":{"id":"xhlRclSlWm_v","colab_type":"text"},"source":["# Tokenizer"]},{"cell_type":"code","metadata":{"id":"fE1Cl3MAyvtN","colab_type":"code","outputId":"e85137cb-1132-4a54-87d2-8c0b5e63b934","executionInfo":{"status":"ok","timestamp":1584915539839,"user_tz":0,"elapsed":1032,"user":{"displayName":"David Massegur","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPdhI-xRflKGWfi6g6h7PuXGRiA48J_SQUfTBPGw=s64","userId":"15590998365704503071"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","print(tf.__version__)\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","sentences = [\n","             'i love my dog',\n","             'I, love my cat',\n","             'You love my dog!',\n","             'Do you think my dog is amazing?'\n","]\n","\n","# tokenizing our text into a dictionary of values: one distinct word -> one distinct integer.\n","tokenizer = Tokenizer(num_words=100)   # it will put in the dictionary the first 100 words in volume appearing in our text.\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","print(word_index)\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["2.1.0\n","{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ukPuYGh4Wped","colab_type":"text"},"source":["# Converting sentences to sequences"]},{"cell_type":"code","metadata":{"id":"jB9_YKm5VIwi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"a7d12359-7248-4bd6-9333-6fea8b406713","executionInfo":{"status":"ok","timestamp":1584915652236,"user_tz":0,"elapsed":667,"user":{"displayName":"David Massegur","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPdhI-xRflKGWfi6g6h7PuXGRiA48J_SQUfTBPGw=s64","userId":"15590998365704503071"}}},"source":["# Convert the sentences into sequences of integers using the dictionary:\n","sequences = tokenizer.texts_to_sequences(sentences)\n","\n","print(sequences)\n","print(sequences[0])  # selecting one sentence"],"execution_count":4,"outputs":[{"output_type":"stream","text":["[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n","[4, 2, 1, 3]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ltb41EVzVass","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"cf033d8a-5681-4415-ae0a-b9449ef0fe89","executionInfo":{"status":"ok","timestamp":1584915751645,"user_tz":0,"elapsed":481,"user":{"displayName":"David Massegur","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPdhI-xRflKGWfi6g6h7PuXGRiA48J_SQUfTBPGw=s64","userId":"15590998365704503071"}}},"source":["test_data = [\n","              'i really love my dog',\n","              'my dog loves my manatee'\n","]\n","\n","test_seq = tokenizer.texts_to_sequences(test_data)\n","\n","print(test_seq)\n","\n","# Result: some words are lost because we are using a dictionry (word_index from above) that didn't have those words.\n","\n","# Thus, we need a broad training data to have a large dictionary."],"execution_count":6,"outputs":[{"output_type":"stream","text":["[[4, 2, 1, 3], [1, 3, 1]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5klL2HLlWV9B","colab_type":"text"},"source":["Result: some words are lost because we are using a dictionry (word_index from above) that didn't have those words.\n","\n","Thus, we need a broad training data to have a large dictionary."]},{"cell_type":"markdown","metadata":{"id":"vQX9mtAHWs2h","colab_type":"text"},"source":["# Indexing unseen words:"]},{"cell_type":"code","metadata":{"id":"dd0EbUvfWwDC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":108},"outputId":"c2e2f56c-2275-4564-fdad-e8129299a984","executionInfo":{"status":"ok","timestamp":1584916196617,"user_tz":0,"elapsed":558,"user":{"displayName":"David Massegur","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPdhI-xRflKGWfi6g6h7PuXGRiA48J_SQUfTBPGw=s64","userId":"15590998365704503071"}}},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","print(tf.__version__)\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","sentences = [\n","             'i love my dog',\n","             'I, love my cat',\n","             'You love my dog!',\n","             'Do you think my dog is amazing?'\n","]\n","\n","# tokenizing our text into a dictionary of values: one distinct word -> one distinct integer.\n","tokenizer = Tokenizer(num_words=100, oov_token='<OOV>')   # it will put in the dictionary the first 100 words in volume appearing in our text. \n","                                                          # the unseen words will be tokenized as OOV (out of vocabulary) via the oov_token option.\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","print(word_index)\n","\n","\n","# Convert the sentences into sequences of integers using the dictionary:\n","sequences = tokenizer.texts_to_sequences(sentences)\n","\n","print(sequences)\n","print(sequences[0])  # selecting one sentence\n","\n","\n","test_data = [\n","              'i really love my dog',\n","              'my dog loves my manatee'\n","]\n","\n","test_seq = tokenizer.texts_to_sequences(test_data)\n","\n","print(test_seq)\n","\n","# Result: the unseen words from the dictionary are tokenized as OOV.\n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["2.1.0\n","{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n","[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n","[5, 3, 2, 4]\n","[[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YHd13OvUYBCH","colab_type":"text"},"source":["# Padding\n","\n","Padding is used to make each sentence sequence of the same length.\n","This is so that all training sentences are of same length to be able to do the NN training."]},{"cell_type":"code","metadata":{"id":"3iib3yPXYMWI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":399},"outputId":"cf6b7e23-210b-4cf3-c249-23e04f63ed3a","executionInfo":{"status":"ok","timestamp":1584917584148,"user_tz":0,"elapsed":515,"user":{"displayName":"David Massegur","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPdhI-xRflKGWfi6g6h7PuXGRiA48J_SQUfTBPGw=s64","userId":"15590998365704503071"}}},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","print(tf.__version__)\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","sentences = [\n","             'i love my dog',\n","             'I, love my cat',\n","             'You love my dog!',\n","             'Do you think my dog is amazing?'\n","]\n","\n","# Tokenizing our text into a Dictionary of values: one distinct word -> one distinct integer.\n","tokenizer = Tokenizer(num_words=100, oov_token='<OOV>')   # it will put in the dictionary the first 100 words in volume appearing in our text. \n","                                                          # the unseen words will be tokenized as OOV (out of vocabulary) via the oov_token option.\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","print('\\nDictionary (word index) =', word_index)\n","\n","\n","# Convert the sentences into sequences of integers using the dictionary:\n","sequences = tokenizer.texts_to_sequences(sentences)\n","\n","print('\\nTrain Sequences =', sequences)\n","print('\\nTrain Sequence #1 =', sequences[0])  # selecting one sentence\n","\n","\n","# Padding consists of converting the sequences all to the same length:\n","padded_seqs = pad_sequences(sequences)\n","# padded_seqs = pad_sequences(sequences, padding='post', maxlen=5)  # padding='post' adds the zeros at the end\n","#                                                                   # maxlen=5 sets the sentence length to 5 and longer sentences are cut off from the beginning.\n","# padded_seqs = pad_sequences(sequences, padding='post', truncating='post', maxlen=5)  # truncating='post' cuts sentences longer than 5 from the end.\n","print('\\nPadded Sequences =')\n","print(padded_seqs)  # the list of sentences has been padded out into a matrix of integers\n","\n","print('\\nPadded train data Dimensions =', padded_seqs.shape)\n","\n","\n","# Tokenizing the test data:\n","test_data = [\n","              'i really love my dog',\n","              'my dog loves my manatee'\n","]\n","\n","test_seq = tokenizer.texts_to_sequences(test_data)\n","\n","print('\\nTest Sequences =',test_seq)\n","\n","\n","# Padding the test data but with same row length as the train data -> using maxlen:\n","padded_test_seqs = pad_sequences(test_seq, maxlen=padded_seqs.shape[1])\n","\n","print('\\nPadded Test Sequences =')\n","print(padded_test_seqs)\n","\n"],"execution_count":24,"outputs":[{"output_type":"stream","text":["2.1.0\n","\n","Dictionary (word index) = {'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n","\n","Train Sequences = [[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n","\n","Train Sequence #1 = [5, 3, 2, 4]\n","\n","Padded Sequences =\n","[[ 0  0  0  5  3  2  4]\n"," [ 0  0  0  5  3  2  7]\n"," [ 0  0  0  6  3  2  4]\n"," [ 8  6  9  2  4 10 11]]\n","\n","Padded train data Dimensions = (4, 7)\n","\n","Test Sequences = [[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n","\n","Padded Test Sequences =\n","[[0 0 5 1 3 2 4]\n"," [0 0 2 4 1 2 1]]\n"],"name":"stdout"}]}]}